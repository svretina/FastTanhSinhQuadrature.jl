var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"Comparison of FastTanhSinhQuadrature.jl vs FastGaussQuadrature.jl.\n\nSystem:\n\nCPU: Intel(R) Core(TM) Ultra 7 155U\nThreads: 1 (Single-threaded execution)","category":"section"},{"location":"benchmarks/#Results","page":"Benchmarks","title":"Results","text":"Legend:\n\nTS: FastTanhSinhQuadrature.integrate\nTS SIMD: FastTanhSinhQuadrature.integrate_avx (using LoopVectorization)\nGQ: FastGaussQuadrature.gausslegendre\n\nFunction Domain Points TS (ns) TS SIMD (ns) GQ (ns) Ratio (TS/GQ) Ratio (TS SIMD/GQ)\nexp(x) [-1, 1] 9 54.24 18.98 46.01 1.18 0.41\nexp(x) [-1, 1] 100 656.00 151.80 310.92 2.11 0.49\nexp(x) [-1, 1] 1000 6941.80 1530.50 3075.75 2.26 0.50\nsin(x)^2 [-1, 1] 9 74.93 41.13 46.04 1.63 0.89\nsin(x)^2 [-1, 1] 100 795.19 287.41 353.35 2.25 0.81\nsin(x)^2 [-1, 1] 1000 8615.33 2749.33 3958.00 2.18 0.69\n1/(1+25x^2) [-1, 1] 9 8.47 5.42 21.22 0.40 0.26\n1/(1+25x^2) [-1, 1] 100 83.82 44.54 77.23 1.09 0.58\n1/(1+25x^2) [-1, 1] 1000 1027.28 466.15 612.57 1.68 0.76\nsqrt(1-x^2) [-1, 1] 9 12.04 8.02 26.30 0.46 0.31\nsqrt(1-x^2) [-1, 1] 100 127.94 68.33 144.65 0.88 0.47\nsqrt(1-x^2) [-1, 1] 1000 1396.40 625.68 1528.60 0.91 0.41\nx^2 [-1, 1] 9 5.12 3.98 20.59 0.25 0.19\nx^2 [-1, 1] 100 34.88 9.74 69.44 0.50 0.14\nx^2 [-1, 1] 1000 442.34 117.87 578.60 0.76 0.20\nx^3 [-1, 1] 9 6.26 3.96 23.39 0.27 0.17\nx^3 [-1, 1] 100 38.67 11.80 65.03 0.59 0.18\nx^3 [-1, 1] 1000 418.34 106.36 540.83 0.77 0.20\nx^3+x^2+x+1 [-1, 1] 9 8.66 4.62 23.41 0.37 0.20\nx^3+x^2+x+1 [-1, 1] 100 89.29 17.90 71.05 1.26 0.25\nx^3+x^2+x+1 [-1, 1] 1000 915.95 180.46 560.56 1.63 0.32\nlog(1-x) [-1, 1] 9 75.40 53.18 56.30 1.34 0.94\nlog(1-x) [-1, 1] 100 752.70 327.72 458.73 1.64 0.71\nlog(1-x) [-1, 1] 1000 7470.00 3503.38 3730.50 2.00 0.94","category":"section"},{"location":"benchmarks/#Analysis","page":"Benchmarks","title":"Analysis","text":"Polynomials: FastTanhSinhQuadrature with SIMD acceleration (x^2, x^3) is significantly faster (up to ~3-5x) than Gauss-Legendre quadrature.\nSingularities: Functions like sqrt(1-x^2) and log(1-x) are handled handled very efficiently, often matching or outperforming Gaussian quadrature due to the double exponential clustering of nodes.\nRunge Function: 1/(1+25x^2) also shows competitive performance, especially with SIMD.\n\nIn summary, for smooth analytic functions, Gaussian quadrature (standard non-SIMD integrate vs GQ) is faster due to fewer nodes required for exactness using polynomials. However, FastTanhSinhQuadrature's SIMD implementation often bridges or exceeds this gap, and it is the superior choice for singular integrands.","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"Pages = [\"api.md\"]","category":"section"},{"location":"api/#FastTanhSinhQuadrature.adaptive_integrate-Union{Tuple{T}, Tuple{Type{T}, Function, Any, Any}} where T<:Real","page":"API Reference","title":"FastTanhSinhQuadrature.adaptive_integrate","text":"adaptive_integrate(::Type{T}, f::Function, a, b; tol::Real=1e-8, max_n::Int=10) where {T<:Real}\n\nAdaptive integration of f over [a, b] using Tanh-Sinh quadrature.  It doubles the number of quadrature points until the relative error is below tol.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.integrate-Union{Tuple{S}, Tuple{T}, Tuple{S, T, T, AbstractVector{T}, AbstractVector{T}, T}} where {T<:Real, S}","page":"API Reference","title":"FastTanhSinhQuadrature.integrate","text":"integrate(f::S, xmin::T, xmax::T, x::AbstractVector{T}, w::AbstractVector{T}, h::T) where {T<:Real,S}\n\nIntegrate function f over [xmin, xmax] using pre-computed nodes x, weights w, and step size h.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.integrate-Union{Tuple{T}, Tuple{Type{T}, Function, Int64}} where T<:Real","page":"API Reference","title":"FastTanhSinhQuadrature.integrate","text":"integrate(::Type{T}, f::Function, n::Int) where {T<:Real}\n\nIntegrate function f over [-1, 1] using Tanh-Sinh quadrature with level n and precision T.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.integrate-Union{Tuple{X}, Tuple{T}, Tuple{X, AbstractVector{T}, AbstractVector{T}, T}} where {T<:Real, X}","page":"API Reference","title":"FastTanhSinhQuadrature.integrate","text":"integrate(f::X, x::AbstractVector{T}, w::AbstractVector{T}, h::T) where {T<:Real,X}\n\nIntegrate function f over [-1, 1] using pre-computed nodes x, weights w, and step size h.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.integrate_avx-Union{Tuple{S}, Tuple{T}, Tuple{S, T, T, AbstractVector{T}, AbstractVector{T}, T}} where {T<:Real, S}","page":"API Reference","title":"FastTanhSinhQuadrature.integrate_avx","text":"integrate_avx(f::S, xmin::T, xmax::T, x::AbstractVector{T}, w::AbstractVector{T}, h::T) where {T<:Real,S}\n\nSIMD-accelerated integration using LoopVectorization. Ensure f is compatible with @turbo.\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.quad-Union{Tuple{X}, Tuple{T}, Tuple{X, T, T, AbstractVector{T}, AbstractVector{T}, T}} where {T<:Real, X}","page":"API Reference","title":"FastTanhSinhQuadrature.quad","text":"quad(f::X, xmin::T, xmax::T, x::AbstractVector{T}, w::AbstractVector{T}, h::T) where {T<:Real,X}\n\nWrapper for integrate that handles domain checks such as xmin > xmax (flips sign) or xmin == xmax (returns 0).\n\n\n\n\n\n","category":"method"},{"location":"api/#FastTanhSinhQuadrature.tanhsinh-Union{Tuple{T}, Tuple{Type{T}, Int64}} where T<:AbstractFloat","page":"API Reference","title":"FastTanhSinhQuadrature.tanhsinh","text":"tanhsinh(::Type{T}, n::Int) where {T<:AbstractFloat}\n\nGenerate Tanh-Sinh quadrature nodes x, weights w, and step size h for a given floating point type T and level n. The number of points generated is approximately 2^n.\n\n\n\n\n\n","category":"method"},{"location":"theory/#Tanh-Sinh-Quadrature","page":"Theory","title":"Tanh-Sinh Quadrature","text":"The quadrature computes integrals of the form\n\nmathcalI=int_-1^1 f(x) dx\n\nThe method is based on a variable transformation which maps the original domain x in (-11) onto the entire real axis t in (-infty infty) using the transformation:\n\nx = Psi(t) = tanhleft(fracpi2 sinh tright)\n\nThe derivative (Jacobian) of this transformation is:\n\nPsi(t) = fracfracpi2 cosh tcosh^2left(fracpi2 sinh tright)\n\nThe integral then becomes:\n\nmathcalI = int_-infty^infty g(t) dt quad g(t)= f(Psi(t)) Psi(t)\n\nSince the method is specific for the x in (-11) domain, one must cast the desired integral to this domain by the linear substitution:\n\nx(u)=fracb+a2+fracb-a2u\n\nThis transformation changes an arbitrary interval ab to -11, hence\n\nint_a^b f(x)dx= fracb-a2int_-1^1 f(x(u))du = fracb-a2int_-infty^infty f(x(u(t))) w(t) dt","category":"section"},{"location":"theory/#Transformation-Visualization","page":"Theory","title":"Transformation Visualization","text":"The key to the Tanh-Sinh quadrature's effectiveness lies in how the transformation maps the integration points. While the discretization in the transformed t-domain uses equidistant notes (t_i = ih), the mapping x = tanh(fracpi2 sinh t) causes these corresponding x_i nodes to cluster double exponentially fast near the endpoints -1 and +1 of the original domain. This dense clustering allows the quadrature to accurately resolve functions even when they have singularities at the boundaries, as the weights decay rapidly enough to suppress the singularity.\n\n(Image: Transformation Visualization) Figure 1: Visualization of the Tanh-Sinh variable transformation. Source: arXiv:2007.15057.","category":"section"},{"location":"theory/#Discretization","page":"Theory","title":"Discretization","text":"We approximate the infinite integral using the trapezoidal rule with step size h:\n\nmathcalI_h = sum_i=-infty^infty h g(t_i) = sum_i=-infty^infty h Psi(t_i) f(Psi(t_i))\n\nwhere t_i = ih. Because the transformed integrand g(t) decays double exponentially (like exp(-fracpi2 e^t)) as t to infty, we can truncate the infinite sum to a finite window -t_n t_n with negligible error:\n\nmathcalI approx Q_h^n = sum_i=-n^n h Psi(t_i) f(Psi(t_i))","category":"section"},{"location":"theory/#Error-Estimation-and-Convergence","page":"Theory","title":"Error Estimation and Convergence","text":"For an integrand f(x) that is regular in a strip of width d in the complex plane around the interval -1 1, the error of the tanh-sinh quadrature decreases exponentially with the number of evaluation points N = 2n+1. Specifically, the error is of the order:\n\nmathcalI - Q_h^n approx mathcalOleft(expleft(-fracpi d Nln(2 d N)right)right)\n\nThis rapid convergence rate is the hallmark of double exponential formulas.","category":"section"},{"location":"theory/#Optimal-Step-Size-and-Truncation","page":"Theory","title":"Optimal Step Size and Truncation","text":"The choice of the step size h and the number of points N are coupled. To balance the discretization error (from the trapezoidal rule) and the truncation error (from cutting off the infinite sum), the optimal step size h for a given N is approximately:\n\nh_opt approx frac2N ln(pi d N)\n\nHowever, in floating-point arithmetic, we are limited by the machine precision. We cannot transform points arbitrarily close to pm 1 without hitting the underflow limit or precision bound of the floating-point type.\n\nThis leads to a maximal step size constraint to ensure numerical stability. If t_max is the largest argument such that we can still distinguish Psi(t_max) from 1 (or weights from 0), then we must have:\n\nh_max = fract_maxn\n\nTypically, t_max is determined by the condition where the weights Psi(t) underflow to zero or the nodes Psi(t) become indistinguishable from pm 1 in the given precision.","category":"section"},{"location":"theory/#Numerical-Stability-Notes","page":"Theory","title":"Numerical Stability Notes","text":"When dealing with finite precision floating point numbers, numerical instabilities can arise.\n\nThe quadrature scheme depends crucially on the evaluations very close to the end-points of the integration domain. The most important cause of numerical instabilities is numerical underflow.\n\nBoth the smallest weight and abscissa value are determined by the window size t_n. The smallest positive normalized floating point number is F_min=2^L.\n\nFor the weights to avoid the numerical underflow we need:\n\nt_max^w = max t Psi(t) geq F_min\n\nSimilarly the smallest abscissa should exceed the machine epsilon / underflow limit relative to the endpoint:\n\nt_max^x = max t  t leq Psi^-1(1-F_min)\n\nSince these conditions need to be satisfied simultaneously we introduce:\n\nt_max^xw = min t_max^x t_max^w \n\nIt is crucial to ensure h is chosen such that nh le t_max^xw.","category":"section"},{"location":"examples/advanced/#Advanced-and-Multidimensional-Integration","page":"Advanced Usage","title":"Advanced & Multidimensional Integration","text":"","category":"section"},{"location":"examples/advanced/#1.-Multidimensional-Integration-(2D,-3D)","page":"Advanced Usage","title":"1. Multidimensional Integration (2D, 3D)","text":"FastTanhSinhQuadrature.jl supports multidimensional integration natively.\n\nusing FastTanhSinhQuadrature\nusing StaticArrays\n\n# Define a 2D function f(x, y) = x^2 + y^2\nf_2d(x, y) = x^2 + y^2\n\n# Integration bounds: [-1, 1] x [-1, 1]\nlow = SVector(-1.0, -1.0)\nup  = SVector(1.0, 1.0)\n\n# Generate quadrature scaling\nx, w, h = tanhsinh(Float64, 10)\n\nval_2d = integrate(f_2d, low, up, x, w, h)\nprintln(\"2D Integral: $val_2d\") ","category":"section"},{"location":"examples/advanced/#2.-Pre-computing-Nodes-for-Performance","page":"Advanced Usage","title":"2. Pre-computing Nodes for Performance","text":"For performance-critical code where you integrate many functions or run loops, always pre-calculate the quadrature nodes (x, w, h).\n\n# Pre-calculate once (expensive operation)\nx, w, h = tanhsinh(Float64, 15)\n\n# Reuse many times (cheap operation)\nfor i in 1:100\n    param = i / 100.0\n    f(t) = exp(-param * t^2)\n    val = integrate(f, 0.0, 10.0, x, w, h)\n    # ... use val\nend","category":"section"},{"location":"examples/advanced/#3.-SIMD-Acceleration-with-integrate_avx","page":"Advanced Usage","title":"3. SIMD Acceleration with integrate_avx","text":"For functions compatible with LoopVectorization.jl, you can achieve significant speedups.\n\nusing FastTanhSinhQuadrature\n\nf_poly(x) = x^12 + 3x^5 - 2x\n\nx, w, h = tanhsinh(Float64, 50) # Heavy quadrature\n\n# Standard\n@time integrate(f_poly, x, w, h)\n\n# AVX Optimized\n@time integrate_avx(f_poly, x, w, h) ","category":"section"},{"location":"examples/basics/#Basic-Usage-Examples","page":"Basic Usage","title":"Basic Usage Examples","text":"This section provides practical examples for using FastTanhSinhQuadrature.jl in common scenarios.","category":"section"},{"location":"examples/basics/#1.-Simple-1D-Integration","page":"Basic Usage","title":"1. Simple 1D Integration","text":"Integrating standard mathematical functions is straightforward.\n\nusing FastTanhSinhQuadrature\n\n# Function to integrate: f(x) = exp(-x^2)\nf(x) = exp(-x^2)\n\n# Integrate over [-1, 1]\nresult = integrate(f, 10) # 10 levels of recursion\nprintln(\"Integral of exp(-x^2) on [-1, 1]: $result\")\n\nIntegrating over an arbitrary interval a b:\n\n# Integrate sin(x) from 0 to pi\nresult_sin = integrate(sin, 0.0, π, 10)\nprintln(\"Integral of sin(x) on [0, π]: $result_sin\") # Should be 2.0","category":"section"},{"location":"examples/basics/#2.-High-Precision-Integration-(Double64,-BigFloat)","page":"Basic Usage","title":"2. High Precision Integration (Double64, BigFloat)","text":"One of the main strengths of Tanh-Sinh quadrature is its ability to handle high-precision arithmetic efficiently.\n\nusing FastTanhSinhQuadrature\nusing DoubleFloats\n\nf(x) = exp(x)\n\n# Use Double64 for extended precision\n# N=12 typically gives ~32 digits of precision\nx, w, h = tanhsinh(Double64, 12)\n\n# Integrate exp(x) on [0, 1]\nval = integrate(f, 0.0, 1.0, x, w, h)\nprintln(\"High precision result: $val\")","category":"section"},{"location":"examples/basics/#3.-Dealing-with-Singularities","page":"Basic Usage","title":"3. Dealing with Singularities","text":"Tanh-Sinh quadrature excels at handling endpoint singularities effectively because the quadrature nodes approach the endpoints exponentially fast but never reach them.","category":"section"},{"location":"examples/basics/#Logarithmic-Singularity-log(1-x)","page":"Basic Usage","title":"Logarithmic Singularity log(1-x)","text":"This function has a singularity at x=1.\n\nf_sing(x) = log(1-x)\n\n# Integrate on [-1, 1]\n# The singularity at x=1 is automatically handled\nval = integrate(f_sing, 10) \nprintln(\"Integral of log(1-x) on [-1, 1]: $val\")","category":"section"},{"location":"examples/basics/#Inverse-Square-Root-1/sqrt(x)-at-x0","page":"Basic Usage","title":"Inverse Square Root 1/sqrt(x) at x=0","text":"# Integrate 1/sqrt(x) from 0 to 1\nf_sqrt(x) = 1.0 / sqrt(x)\n\nx, w, h = tanhsinh(Float64, 10)\nval = integrate(f_sqrt, 0.0, 1.0, x, w, h)\nprintln(\"Integral of 1/sqrt(x) on [0, 1]: $val\") # Should be 2.0","category":"section"},{"location":"examples/basics/#4.-Adaptive-Integration","page":"Basic Usage","title":"4. Adaptive Integration","text":"If you require a specific tolerance rather than specifying a fixed number of points, use adaptive_integrate.\n\n# Integrate generic function to 1e-12 tolerance\nval = adaptive_integrate(x -> cos(x)^2, 0.0, 2π, tol=1e-12)\nprintln(val)","category":"section"},{"location":"#FastTanhSinhQuadrature.jl","page":"Home","title":"FastTanhSinhQuadrature.jl","text":"<img src=\"assets/logo.svg\" alt=\"FastTanhSinhQuadrature Logo\" width=\"400\">\n\n(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)\n\nFastTanhSinhQuadrature.jl is a high-performance Julia library for numerical integration using the Tanh-Sinh (Double Exponential) quadrature method.\n\nIt handles singularities at endpoints robustly, supports arbitrary precision arithmetic (e.g., BigFloat, Double64), and leverages SIMD for speed.","category":"section"},{"location":"#Usage-at-a-Glance","page":"Home","title":"Usage at a Glance","text":"using FastTanhSinhQuadrature\n\n# 1. Define function\nf(x) = x * exp(x)\n\n# 2. Integrate on [-1, 1]\nval = integrate(f, 10) # 10 levels\nprintln(val)","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Theory: Understand the mathematics behind the method.\nBasic Examples: Learn how to integrate simple 1D functions.\nAdvanced Examples: Multidimensional integration and performance tips.\nBenchmarks: Performance comparison against other libraries.\nAPI Reference: Detailed function documentation.\n\n","category":"section"}]
}
